[
  {
    "slug": "abeer-alwan",
    "name": "Abeer Alwan",
    "affiliation": "UCLA",
    "bio": "Abeer Alwan received her Ph.D. in Electrical Engineering and Computer Science from MIT in 1992. Since then, she has been with the UCLA ECE department where she is now a Distinguished Professor and directs the Speech Processing and Auditory Perception Laboratory (http://www.seas.ucla.edu/spapl/). Dr. Alwan’s research interests are in the areas of speech production and perception modeling and applications to speech technology such as automatic speech recognition, speaker identification and text-to-speech synthesis. Her focus is on limited data or low-resource systems where knowledge of speech production and perception, and linguistics can be critical to system performance. Current projects include detecting depression from speech signals, children’s speech recognition, speaker recognition with limited data, and the recognition of low-resource dialects. She is the recipient of several awards including the NSF Research Initiation and Career Awards, NIH FIRST Award, UCLA-TRW Excellence in Teaching Award, Okawa Foundation Award in Telecommunication, and the Engineer’s Council Educator Award. She is a Fellow of the Acoustical Society of America, IEEE, and the International Speech Communication Association (ISCA). She was a Fellow at the Radcliffe Institute, Harvard University, co-Editor in Chief of Speech Communication, and Associate Editor of both JASA and IEEE TSALP.",
    "role": "Keynote Speaker",
    "image": "https://drive.google.com/file/d/1I2bumhg3WWQye2wAVGGBtEelaGFVcK5I/view?usp=drive_link",
    "website": "https://www.ee.ucla.edu/abeer-alwan/",
    "talkTitle": "Advances and Challenges of Child ASR",
    "talkDescription": "Child speech is characterized by larger inter- and intra- speaker variability than adults’ speech, partly due to vocal tract changes as children grow. In addition, there is a lack of large, publicly available datasets that can adequately train machine learning algorithms for various recognition tasks. As a result, the performance of automatic speech recognition (ASR) systems of child speech is worse than that of adults. In this talk, I will summarize various efforts in data collection, developing data augmentation techniques, and benchmarking children’s speech recognition with supervised and self-supervised speech foundation models. Our studies point to the need for accounting for several factors when designing child speech processing systems: age (an ASR system that works well for a 9-year-old child would not necessarily work well for a 6-year-old),  style (reading versus spontaneous speech),  dialect (differences not only in pronunciation but also in word usage and grammar), and reading and/or language impairment. Moreover, for language assessments, transliteration is sometimes more valuable to the teacher than a corrected transcription. As a result, data diversity, and not just quantity, is especially critical when designing child ASR systems. While significant progress has been made in child speech processing, several challenges remain.",
    "time": "14:05–14:35",
    "room": "CC308"
  },
  {
    "slug": "mark-hasegawa-johnson",
    "name": "Mark Hasegawa-Johnson",
    "affiliation": "UIUC",
    "bio": "Mark A. Hasegawa-Johnson received his B.S. M.S. and Ph.D. in Electrical Engineering from MIT in 1996.  He was a postdoctoral researcher at University of California, Los Angeles and is now the M.E. Van Valkenburg Professor of Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign, Illinois, 61801, U.S.A. Dr. Hasegawa-Johnson is currently Editor in Chief of the IEEE Transactions on Audio, Speech, and Language Processing. His research interests include automatic speech understanding as an empowerment tool for parents, children, learners, and people with disabilities, and unsupervised learning methods for speech technology. He is a Fellow of the IEEE, a Fellow of the ASA, and a Fellow of ISCA. ",
    "role": "Invited Speaker",
    "image": "https://drive.google.com/file/d/1wHuxAlizjJ3o_imZrR3IQCLVdmHOAGNv/view?usp=drive_link",
    "website": "https://speechtechnology.web.illinois.edu/mark-a-hasegawa-johnson/",
    "talkTitle": "Speech as a modality for the characterization and adaptation of neurodiversity",
    "talkDescription": "Parents without medical expertise may seek help to integrate their children into home life, school, and society. Neuromotor conditions such as cerebral palsy (CP) and Down syndrome (DS) are typically diagnosed prenatally or at birth, but may generate challenges later; conditions such as apnea, anxiety, autism, and developmental language delay may remain undetected until their behavioral correlates have caused problems. Artificial intelligence has the potential to characterize neurodiversity early in life, and to adapt its behavior in order to help the child and her parents learn together. Wearables such as Littlebeats (TM) have been shown to discriminate sleep versus waking, and monologue versus dialogue infant vocalizations; with these abilities, an infant wearable has the potential to detect behavioral disorders early in life, and to help the parents find accommodations. Accurate tests of developmental language delay exist for children 3-5 years of age, and professional speech and language treatments have been shown to improve outcomes: automatic speech recognition (ASR) for young children has the potential to make these treatments available to all children who need them. Thanks to the Speech Accessibility Project, ASR error rates for adults with Parkinson's disease halved this year, and there is reason to believe that similarly large improvements for adults with CP and DS could help grant them better access to economic and social opportunities.  In these and other ways, artificially intelligent speaking agents have the potential to bridge gaps in society, and improve inter-human interaction.",
    "time": "14:35–15:05",
    "room": "CC308"
  },
  {
    "slug": "gopala-anumanchipalli",
    "name": "Gopala Anumanchipalli",
    "affiliation": "UC Berkeley",
    "bio": "",
    "role": "Invited Speaker",
    "image": "https://drive.google.com/file/d/1b3IC0uq_OoG-iOtFchbJwJyfxkdWo6Yr/view?usp=drive_link",
    "website": "https://vcresearch.berkeley.edu/faculty/gopala-krishna-anumanchipalli",
    "talkTitle": "Invited Talk 2 – [TBD]",
    "talkDescription": "Invited talk. Title to be announced.",
    "time": "15:05–15:35",
    "room": "CC308"
  },
  {
    "slug": "emma-neill",
    "name": "Emma O'Neill",
    "affiliation": "Curriculum Associates",
    "bio": "",
    "role": "Invited Speaker",
    "image": "https://drive.google.com/file/d/1t5m2ZmsGdl02llZfsliU_-WhIjaarDib/view?usp=drive_link",
    "website": "https://www.linkedin.com/in/emmaloneill/",
    "talkTitle": "Invited Talk 3 – [TBD]",
    "talkDescription": "Invited talk. Title to be announced.",
    "time": "15:50–16:20",
    "room": "CC308"
  },
  {
    "slug": "tiantian-feng",
    "name": "Tiantian Feng",
    "affiliation": "USC",
    "bio": "",
    "role": "Invited Speaker",
    "image": "https://drive.google.com/file/d/1WdhbpSAPm9xxft2bHtHNq1UHoZT4dIV_/view?usp=drive_link",
    "website": "https://tiantiaf0627.github.io/",
    "talkTitle": "Invited Talk 4 – [TBD]",
    "talkDescription": "Invited talk. Title to be announced.",
    "time": "16:20–16:50",
    "room": "CC308"
  }
]
